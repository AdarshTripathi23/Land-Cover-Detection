{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Land Cover Classification of Satellite Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aidan O'Keefe**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMAGE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A deep learning (neural network) land cover classification project using satellite images (remote sensing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import needed libraries\n",
    "import os, shutil\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "#Standard Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualizations\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.data.experimental import cardinality\n",
    "from tensorflow.keras.utils import to_categorical \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense # creates densely connected layer object\n",
    "from tensorflow.keras.layers import Flatten # takes 2D input and turns into 1D array\n",
    "from tensorflow.keras.layers import Conv2D # convolution layer\n",
    "from tensorflow.keras.layers import MaxPooling2D # max pooling layer\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up directory path to image files\n",
    "image_path = '/Users/Aidan/Documents/Flatiron/Phase_5/EuroSAT_RGB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Images into Train, Test, and Validation folders\n",
    "# OS and Shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21600 images belonging to 10 classes.\n",
      "Found 5400 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# Normalize images\n",
    "image_gen = ImageDataGenerator(rescale=1./255, validation_split = 0.2)\n",
    "\n",
    "#Import data as 80% train and 20% test\n",
    "train_generator = image_gen.flow_from_directory(image_path,\n",
    "                                                class_mode = 'categorical', \n",
    "                                                subset ='training', \n",
    "                                                batch_size=128,\n",
    "                                                  shuffle=True,\n",
    "                                                seed=42)\n",
    "                                               \n",
    "test_generator= image_gen.flow_from_directory(image_path,\n",
    "                                               class_mode= 'categorical',\n",
    "                                               subset = \"validation\",\n",
    "                                                batch_size=128,\n",
    "                                                shuffle=True,\n",
    "                                               seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ~ [(0, 2400), (1, 2400), (2, 2400), (3, 2000), (4, 2000), (5, 1600), (6, 2000), (7, 2400), (8, 2000), (9, 2400)]\n",
      "Test ~ [(0, 600), (1, 600), (2, 600), (3, 500), (4, 500), (5, 400), (6, 500), (7, 600), (8, 500), (9, 600)]\n"
     ]
    }
   ],
   "source": [
    "#Confirm class balance for train and test\n",
    "\n",
    "train_labels = train_generator.classes\n",
    "test_labels = test_generator.classes\n",
    "\n",
    "train_label, train_count = np.unique(train_labels, return_counts=True)\n",
    "test_label, test_count = np.unique(test_labels, return_counts=True)\n",
    "\n",
    "print('Train ~ {}'.format(list(zip(train_label, train_count))))\n",
    "print('Test ~ {}'.format(list(zip(test_label, test_count))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: {'AnnualCrop': 0, 'Forest': 1, 'HerbaceousVegetation': 2, 'Highway': 3, 'Industrial': 4, 'Pasture': 5, 'PermanentCrop': 6, 'Residential': 7, 'River': 8, 'SeaLake': 9}\n",
      "Train: {'AnnualCrop': 0, 'Forest': 1, 'HerbaceousVegetation': 2, 'Highway': 3, 'Industrial': 4, 'Pasture': 5, 'PermanentCrop': 6, 'Residential': 7, 'River': 8, 'SeaLake': 9}\n"
     ]
    }
   ],
   "source": [
    "#Checking the classes in our train data\n",
    "train_class_names = train_generator.class_indices\n",
    "print('Train:', train_class_names)\n",
    "\n",
    "#Checking the classes in our test data\n",
    "test_class_names = test_generator.class_indices\n",
    "print('Train:', test_class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the number of classes in the train and test data match\n",
    "len(test_class_names) == len(train_class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_images, train_labels = next(train_generator)\n",
    "# test_images, test_labels = next(test_generator)\n",
    "# # val_images, val_labels = next(val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m_train = train_images.shape[0]\n",
    "# num_px = train_images.shape[1]\n",
    "# m_test = test_images.shape[0]\n",
    "# # m_val = val_images.shape[0]\n",
    "\n",
    "# print (\"Number of training samples: \" + str(m_train))\n",
    "# print (\"Number of testing samples: \" + str(m_test))\n",
    "# # print (\"Number of validation samples: \" + str(m_val))\n",
    "# print (\"train_images shape: \" + str(train_images.shape))\n",
    "# print (\"train_labels shape: \" + str(train_labels.shape))\n",
    "# print (\"test_images shape: \" + str(test_images.shape))\n",
    "# print (\"test_labels shape: \" + str(test_labels.shape))\n",
    "# # print (\"val_images shape: \" + str(val_images.shape))\n",
    "# # print (\"val_labels shape: \" + str(val_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_img = train_images.reshape(train_images.shape[0], -1)\n",
    "# test_img = test_images.reshape(test_images.shape[0], -1)\n",
    "# # val_img = val_images.reshape(val_images.shape[0], -1)\n",
    "\n",
    "# print(train_img.shape)\n",
    "# print(test_img.shape)\n",
    "# # print(val_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_y = np.reshape(train_labels[:,0], (128,1))\n",
    "# test_y = np.reshape(test_labels[:,0], (128,1))\n",
    "# # val_y = np.reshape(val_labels[:,0], (200,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 254, 254, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 127, 127, 32)      0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 516128)            0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 64)                33032256  \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 33,033,802\n",
      "Trainable params: 33,033,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Instantiate a Sequential model\n",
    "baseline_model = Sequential()\n",
    "\n",
    "\n",
    "# Input Layer- Convolution\n",
    "baseline_model.add(Conv2D(filters=32,\n",
    "                          kernel_size=(3, 3),\n",
    "                          activation='relu',\n",
    "                          input_shape= (256, 256, 3)))\n",
    "\n",
    "\n",
    "# Layer 1- max pool in 2x2 window\n",
    "baseline_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 2- connect all nodes with dense layer\n",
    "baseline_model.add(Flatten())\n",
    "baseline_model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Output Layer- softmax activiation for multi-categorical with 10 classes\n",
    "baseline_model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "#Compile the sequential CNN model- adam optimizer, \n",
    "# categorical_crossentropy loss, and set our metric to accuracy\n",
    "baseline_model.compile(optimizer='adam', \n",
    "                       loss='categorical_crossentropy',  \n",
    "                       metrics=['accuracy'])\n",
    "\n",
    "# print model summary\n",
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "169/169 [==============================] - 340s 2s/step - loss: 2.6182 - accuracy: 0.3431\n",
      "Epoch 2/5\n",
      "169/169 [==============================] - 360s 2s/step - loss: 1.0948 - accuracy: 0.6223\n",
      "Epoch 3/5\n",
      "169/169 [==============================] - 323s 2s/step - loss: 0.8791 - accuracy: 0.6971\n",
      "Epoch 4/5\n",
      "169/169 [==============================] - 352s 2s/step - loss: 0.7732 - accuracy: 0.7324\n",
      "Epoch 5/5\n",
      "169/169 [==============================] - 319s 2s/step - loss: 0.6601 - accuracy: 0.7756\n"
     ]
    }
   ],
   "source": [
    "#Fit the model with the training data and 20% validation split (16% of original data).\n",
    "baseline_history = baseline_model.fit(train_gen_dataset, \n",
    "                                      epochs = 5, \n",
    "                                      batch_size= 128, \n",
    "                                      verbose = 1) \n",
    "#                                       validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 25s 577ms/step - loss: 0.7964 - accuracy: 0.7154\n",
      "Test loss:  0.7963578701019287\n",
      "Test accuracy:  0.7153703570365906\n"
     ]
    }
   ],
   "source": [
    "#Check loss and accuracy on test data\n",
    "test_loss, test_acc = baseline_model.evaluate(test_gen_dataset, verbose = 1)\n",
    "\n",
    "print('Test loss: ', test_loss)\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like after 5 Epochs our baseline model overfit with a 77.5% accuracy on the train data and 71.5% accuracy on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # probability for each class\n",
    "# y_proba = model_images.predict(x_test)\n",
    "# y_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # argmax axis = -1 gets the column index of maximum probability for each row.\n",
    "# # column index corresponds to digit classes (numbers 0 -9)\n",
    "# predicted = np.argmax(y_proba, axis=-1)\n",
    "# predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #View confusion matrix of test predictions\n",
    "# cm_digits = confusion_matrix(y_test, predicted)\n",
    "# disp = ConfusionMatrixDisplay(\n",
    "#     confusion_matrix=cm_digits)\n",
    "\n",
    "# disp.plot(cmap=plt.cm.Blues)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Instantiate the model\n",
    "# model = Sequential()\n",
    "# # define 3x3 filter window sizes. Create 32 filters.\n",
    "# # COv2D input shape =(image_height, image_width, color_channels) for each image\n",
    "# model.add(Conv2D(filters=32,\n",
    "#                         kernel_size=(3, 3),\n",
    "#                         activation='relu',\n",
    "#                         input_shape=(28, 28, 1)))\n",
    "# #IF STRIDE IS NOT SPECIFIED, IT IS AUTOMATICALLY THE KERNAL/FILTER SIZE. eg. s=3 for 3x3 filter\n",
    "# # max pool in 2x2 window\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# #SAME DEFAULT FOR MAX POOL STRIDE\n",
    "# # define 3x3 filter window sizes. Create 64 filters.\n",
    "# model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D((2, 2)))\n",
    "# model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "# # transition to dense fully-connected part of network\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "# model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Compile the model\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #Define Stopping Criteria \n",
    "# # valcallback = EarlyStopping(monitor='val_loss', mode='min', verbose = 1, patience = 2)\n",
    "\n",
    "\n",
    "# # Fit the model\n",
    "# history_cnn = \\\n",
    "# model.fit(train_images, train_labels, epochs= 20, validation_split = 0.2, batch_size=32, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Evaluate the accuracy of the model on test data\n",
    "# _, test_acc = model.evaluate(test_images, to_categorical(test_labels), verbose =2)\n",
    "# print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
